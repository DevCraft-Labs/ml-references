{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorlayerX for Tabular Data\n",
    "\n",
    "This notebook contains me learning TensorlayerX for tabular data. The case is using Default Credit Classifier based-on my previous ANN project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch;\n",
    "import pandas;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.preprocessing import StandardScaler;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0      20000    2          2         1   24      2      2     -1     -1   \n",
       "1     120000    2          2         2   26     -1      2      0      0   \n",
       "2      90000    2          2         2   34      0      0      0      0   \n",
       "3      50000    2          2         1   37      0      0      0      0   \n",
       "4      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0     -2  ...          0          0          0         0       689         0   \n",
       "1      0  ...       3272       3455       3261         0      1000      1000   \n",
       "2      0  ...      14331      14948      15549      1518      1500      1000   \n",
       "3      0  ...      28314      28959      29547      2000      2019      1200   \n",
       "4      0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  LABEL  \n",
       "0         0         0         0      1  \n",
       "1      1000         0      2000      1  \n",
       "2      1000      1000      5000      0  \n",
       "3      1100      1069      1000      0  \n",
       "4      9000       689       679      0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data from pre-cleaned csv file\n",
    "# current_folder = \"/mnt/d/Code/College/Machine Learning/Team Assignment/Default Credit Scoring/\";\n",
    "current_folder = \"C:\\\\Users\\\\yosua\\\\Downloads\\\\ml-team-project-master\\\\ml-team-project-master\\\\\";\n",
    "# current_folder = \"\";\n",
    "dataframe = pandas.read_csv(current_folder + \"credit_card_clients.csv\");\n",
    "\n",
    "dataframe = dataframe.drop(columns = [\"ID\"]);\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping everything that other than mentioned above\n",
      "Dropping feature SEX since its not \"important\"\n",
      "Dropping feature EDUCATION since its not \"important\"\n",
      "Dropping feature MARRIAGE since its not \"important\"\n",
      "Dropping feature AGE since its not \"important\"\n",
      "Dropping feature BILL_AMT2 since its not \"important\"\n",
      "Dropping feature BILL_AMT3 since its not \"important\"\n",
      "Dropping feature BILL_AMT4 since its not \"important\"\n",
      "Dropping feature BILL_AMT5 since its not \"important\"\n",
      "Dropping feature BILL_AMT6 since its not \"important\"\n",
      "Dropping feature PAY_AMT1 since its not \"important\"\n",
      "Dropping feature PAY_AMT4 since its not \"important\"\n",
      "Dropping feature PAY_AMT5 since its not \"important\"\n",
      "Dropping feature PAY_AMT6 since its not \"important\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Dropping everything that other than mentioned above\");\n",
    "\n",
    "most_important_features = [\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_AMT2\",\n",
    "    \"LIMIT_BAL\",\n",
    "    \"PAY_AMT3\",\n",
    "    \"BILL_AMT1\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_6\",\n",
    "    \"PAY_5\",\n",
    "    \"LABEL\" # Don't :)\n",
    "];\n",
    "\n",
    "columns_to_be_dropped = [];\n",
    "for i in dataframe.columns:\n",
    "    if i not in most_important_features:\n",
    "        columns_to_be_dropped.append(i);\n",
    "        print(f\"Dropping feature {i} since its not \\\"important\\\"\");\n",
    "\n",
    "dataframe = dataframe.drop(columns = columns_to_be_dropped);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after cleaning:  (30000, 75)\n",
      "['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'LABEL', 'pay_0_-2', 'pay_0_-1', 'pay_0_0', 'pay_0_1', 'pay_0_2', 'pay_0_3', 'pay_0_4', 'pay_0_5', 'pay_0_6', 'pay_0_7', 'pay_0_8', 'pay_2_-2', 'pay_2_-1', 'pay_2_0', 'pay_2_1', 'pay_2_2', 'pay_2_3', 'pay_2_4', 'pay_2_5', 'pay_2_6', 'pay_2_7', 'pay_2_8', 'pay_3_-2', 'pay_3_-1', 'pay_3_0', 'pay_3_1', 'pay_3_2', 'pay_3_3', 'pay_3_4', 'pay_3_5', 'pay_3_6', 'pay_3_7', 'pay_3_8', 'pay_4_-2', 'pay_4_-1', 'pay_4_0', 'pay_4_1', 'pay_4_2', 'pay_4_3', 'pay_4_4', 'pay_4_5', 'pay_4_6', 'pay_4_7', 'pay_4_8', 'pay_5_-2', 'pay_5_-1', 'pay_5_0', 'pay_5_2', 'pay_5_3', 'pay_5_4', 'pay_5_5', 'pay_5_6', 'pay_5_7', 'pay_5_8', 'pay_6_-2', 'pay_6_-1', 'pay_6_0', 'pay_6_2', 'pay_6_3', 'pay_6_4', 'pay_6_5', 'pay_6_6', 'pay_6_7', 'pay_6_8']\n"
     ]
    }
   ],
   "source": [
    "# Make one hot encoding for PAY_0 to 6 since the data is an ordinal data\n",
    "hot_encoded_pay_0 = pandas.get_dummies(dataframe['PAY_0'], prefix = \"pay_0\");\n",
    "hot_encoded_pay_2 = pandas.get_dummies(dataframe['PAY_2'], prefix = \"pay_2\");\n",
    "hot_encoded_pay_3 = pandas.get_dummies(dataframe['PAY_3'], prefix = \"pay_3\");\n",
    "hot_encoded_pay_4 = pandas.get_dummies(dataframe['PAY_4'], prefix = \"pay_4\");\n",
    "hot_encoded_pay_5 = pandas.get_dummies(dataframe['PAY_5'], prefix = \"pay_5\");\n",
    "hot_encoded_pay_6 = pandas.get_dummies(dataframe['PAY_6'], prefix = \"pay_6\");\n",
    "\n",
    "# Merge the hot_encoded with the main dataframe\n",
    "for i in [hot_encoded_pay_0, hot_encoded_pay_2, hot_encoded_pay_3, hot_encoded_pay_4, hot_encoded_pay_5, hot_encoded_pay_6]:\n",
    "    dataframe = pandas.concat([dataframe, i], axis = 1);\n",
    "\n",
    "print(\"Data after cleaning: \", dataframe.shape);\n",
    "print(dataframe.columns.tolist());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape:  (30000, 68)\n"
     ]
    }
   ],
   "source": [
    "# Define label data\n",
    "label = dataframe['LABEL'];\n",
    "\n",
    "# Drop ID, SEX, EDUCATION, MARRIAGE, and LABEL from dataframe for features\n",
    "features = dataframe.drop(columns=['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', \"LABEL\"]);\n",
    "\n",
    "# Typecasting all values within to int64 because ANN need to be standardized\n",
    "\n",
    "for i in dataframe.columns:\n",
    "    dataframe[i] = dataframe[i].astype(int);\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "    features, \n",
    "    label, \n",
    "    train_size = 0.7, \n",
    "    test_size = 0.3, \n",
    "    random_state = 42\n",
    ");\n",
    "\n",
    "feature_validation, feature_test, label_validation, label_test = train_test_split(\n",
    "    feature_test, \n",
    "    label_test, \n",
    "    test_size=0.5, \n",
    "    random_state = 42\n",
    ");\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler();\n",
    "feature_train_scaled = scaler.fit_transform(feature_train);\n",
    "feature_validation_scaled = scaler.fit_transform(feature_validation);\n",
    "feature_test_scaled = scaler.transform(feature_test);\n",
    "\n",
    "print(\"Features shape: \", features.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\tensorlayerx\\__init__.py:45: UserWarning: The version of the backend you have installed does not match the specified backend version and may not work, please install version tensorflow 2.4.0.\n",
      "  warnings.warn(\"The version of the backend you have installed does not match the specified backend version \"\n"
     ]
    }
   ],
   "source": [
    "import tensorlayerx;\n",
    "from tensorlayerx.nn import Module, Sequential, Input, Linear, Sigmoid;\n",
    "from tensorlayerx.model import Model;\n",
    "from tensorlayerx.optimizers import Adam;\n",
    "from tensorlayerx.losses import binary_cross_entropy;\n",
    "from tensorlayerx.metrics import acc;\n",
    "from tensorlayerx.dataflow import IterableDataset;\n",
    "\n",
    "\n",
    "import os;\n",
    "os.environ[\"TL_BACKEND\"] = \"pytorch\";\n",
    "\n",
    "import tensorflow;\n",
    "from tensorflow import convert_to_tensor, expand_dims;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader(IterableDataset):\n",
    "    def __init__(self, feature, label):\n",
    "        self.data = expand_dims(convert_to_tensor(feature, dtype = tensorflow.float32), axis = 1);\n",
    "        self.label = convert_to_tensor(label, dtype = tensorflow.float32);\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index];\n",
    "        label = self.label[index];\n",
    "\n",
    "        return data, label;\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.data)):\n",
    "            yield self.data[i], self.label[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] Linear  dense1: 64 Sigmoid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] Linear  dense2: 32 Sigmoid\n",
      "[TLX] Linear  output: 1 Sigmoid\n"
     ]
    }
   ],
   "source": [
    "class ANNTlx (Module):\n",
    "    def __init__(self):\n",
    "        super (ANNTlx, self).__init__();\n",
    "\n",
    "        # The input layer is explicit. Input(shape = [68]);\n",
    "        self.dense1 = Linear(out_features = 64, act = Sigmoid, name = \"dense1\");\n",
    "        self.dense2 = Linear(in_features = 64, out_features = 32, act = Sigmoid, name = \"dense2\", W_init = tensorlayerx.initializers.random_uniform(minval = 0.0, maxval = 0.1));\n",
    "\n",
    "        self.output = Linear(in_features = 32, out_features = 1, act = Sigmoid, name = \"output\");\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x);\n",
    "        x = self.dense2(x);\n",
    "        out = self.output(x);\n",
    "\n",
    "        return out;\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.dense1(x);\n",
    "        x = self.dense2(x);\n",
    "        out = self.output(x);\n",
    "\n",
    "        return out;\n",
    "        \n",
    "network = ANNTlx();\n",
    "loss_function = binary_cross_entropy;\n",
    "metric_function = acc;\n",
    "optimizer = Adam(lr = 1e-3);\n",
    "\n",
    "model = Model(network, loss_fn = loss_function, optimizer = optimizer, metric_function = metric_function);\n",
    "train_dataset = DatasetLoader(feature_train_scaled, label_train);\n",
    "val_dataset = DatasetLoader(feature_validation_scaled, label_validation);\n",
    "test_dataset = DatasetLoader(feature_test_scaled, label_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feeee542a624995bed36feed07924fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\tensorlayerx\\model\\core.py:99\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, n_epoch, train_dataset, test_dataset, print_train_batch, print_freq)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected type in (train_dataset, Iterable), but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(train_dataset)))\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tlx\u001b[38;5;241m.\u001b[39mBACKEND \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_train_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tlx\u001b[38;5;241m.\u001b[39mBACKEND \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmindspore\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mms_train(\n\u001b[0;32m    106\u001b[0m         n_epoch\u001b[38;5;241m=\u001b[39mn_epoch, train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, network\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork, loss_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn,\n\u001b[0;32m    107\u001b[0m         train_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_weights, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics,\n\u001b[0;32m    108\u001b[0m         print_train_batch\u001b[38;5;241m=\u001b[39mprint_train_batch, print_freq\u001b[38;5;241m=\u001b[39mprint_freq, test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[0;32m    109\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\tensorlayerx\\model\\core.py:294\u001b[0m, in \u001b[0;36mModel.tf_train\u001b[1;34m(self, n_epoch, train_dataset, network, loss_fn, train_weights, optimizer, metrics, print_train_batch, print_freq, test_dataset)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;66;03m# compute loss and update model\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     _loss_ce \u001b[38;5;241m=\u001b[39m loss_fn(_logits, y_batch)\n\u001b[1;32m--> 294\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_loss_ce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grad, train_weights))\n\u001b[0;32m    297\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _loss_ce\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1062\u001b[0m           output_gradients))\n\u001b[0;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:118\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    109\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_control_flow_context\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.GradientTape.gradients() does not support graph control flow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperations like tf.cond or tf.while at this time. Use tf.gradients() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead. If you need this feature, please file a feature request at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/tensorflow/tensorflow/issues/new\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m     )\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gradient_function\u001b[39m(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[0;32m    119\u001b[0m                        out_grads, skip_input_indices, forward_pass_name_scope):\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the gradient function of the op.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    The gradients with respect to the inputs of the function, as a list.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m   mock_op \u001b[38;5;241m=\u001b[39m _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(n_epoch = 10, train_dataset = train_dataset, test_dataset = val_dataset, print_freq = 1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
