{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow;\n",
    "import os;\n",
    "\n",
    "os.environ[\"TL_BACKEND\"] = \"tensorflow\";\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\";\n",
    "\n",
    "import tensorlayerx;\n",
    "from tensorflow import convert_to_tensor;\n",
    "import numpy;\n",
    "\n",
    "from tensorflow.keras.datasets import mnist;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow.config.list_physical_devices('GPU'));\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set memory growth for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        # Set memory growth\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"Memory growth set for GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data();\n",
    "\n",
    "# Split the test and val by 50:50\n",
    "test_val_images_split = numpy.array_split(test_images, 2);\n",
    "test_val_labels_split = numpy.array_split(test_labels, 2);\n",
    "\n",
    "test_images = test_val_images_split[0];\n",
    "test_labels = test_val_labels_split[0];\n",
    "\n",
    "val_images = test_val_images_split[1];\n",
    "val_labels = test_val_labels_split[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall step 1\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255;\n",
    "test_images = test_images.reshape((5000, 28, 28, 1)).astype('float32') / 255;\n",
    "val_images = val_images.reshape((5000, 28, 28, 1)).astype('float32') / 255;\n",
    "\n",
    "\n",
    "# Recall step 2\n",
    "train_labels = tensorflow.one_hot(train_labels, depth = 10);\n",
    "test_labels = tensorflow.one_hot(test_labels, depth = 10);\n",
    "val_labels = tensorflow.one_hot(val_labels, depth = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorlayerx.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear, Module, Input, ReLU, Softmax;\n",
    "from tensorlayerx.model import TrainOneStep;\n",
    "from tensorlayerx.optimizers import Adam;\n",
    "from tensorlayerx.losses import cross_entropy_seq;\n",
    "from tqdm import tqdm;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel (Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv2d(out_channels = 32, kernel_size = (3, 3), stride = (1, 1), in_channels = 1, act = ReLU, name = \"conv1\");\n",
    "        # self.pool1 = MaxPool2d(kernel_size = (2, 2), name = \"pool1\");\n",
    "\n",
    "        self.conv2 = Conv2d(out_channels = 64, kernel_size = (1, 1), stride = (1, 1), in_channels = 1, act = ReLU, name = \"conv2\");\n",
    "        # self.pool2 = MaxPool2d(kernel_size = (2, 2), name = \"pool2\");\n",
    "\n",
    "        self.flat = Flatten(name = \"flat\");\n",
    "\n",
    "        self.dense = Linear(out_features = 64, in_features = 50176, act = ReLU, name = \"dense1\");\n",
    "        self.output = Linear(out_features = 10, in_features = 64, act = Softmax, name = \"output\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x);\n",
    "        # x = self.pool1(x);\n",
    "    \n",
    "        x = self.conv2(x);\n",
    "        # x = self.pool2(x);\n",
    "\n",
    "        x = self.flat(x);\n",
    "        x = self.dense(x);\n",
    "\n",
    "        x = self.output(x);\n",
    "    \n",
    "        return x;\n",
    "\n",
    "    \n",
    "    def compile(self):\n",
    "        model = Sequential([\n",
    "            self.conv1,\n",
    "            self.pool1,\n",
    "            self.conv2,\n",
    "            self.pool2,\n",
    "            self.flat,\n",
    "            self.output\n",
    "        ]);\n",
    "\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(\"               MNIST Tensorlayerx               \");\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(\"Neuron: \", sum(layer.out_features for layer in model if hasattr(layer, 'out_features')))\n",
    "        print(model);\n",
    "\n",
    "        return model;\n",
    "\n",
    "class ForwardPropagation(Module):\n",
    "    def __init__(self, neural_network):\n",
    "        super(ForwardPropagation, self).__init__();\n",
    "        self.net = neural_network;\n",
    "        self.loss_fn = cross_entropy_seq;\n",
    "\n",
    "    def forward(self, feature_set, label_set):\n",
    "        out = self.net(feature_set);\n",
    "        loss = self.loss_fn(out, label_set);\n",
    "        return loss;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualTrainer:\n",
    "\n",
    "    \"\"\"\n",
    "    ManualTrainer\n",
    "\n",
    "    This class is mimicing the Tensorflow's automatic fit. \n",
    "    However, since the adversarial training algorithm may requires TrainOneStep.\n",
    "\n",
    "    @params\n",
    "        epoch: number of epoch\n",
    "        batch_size: total data per training minibatch (For every epoch elapsed, train the data with minibatches.)\n",
    "        model: model to be trained (tensorlayerx.nn.Sequential)\n",
    "        optimizer: optimizer algorithm to be used for training (tensorlayerx.optimizers)\n",
    "        loss: loss function to be used for training (tensorlayerx.losses)\n",
    "        \n",
    "        ----------------------------------------------------------------\n",
    "        Every parameters below are defined as array length of 2:\n",
    "        train_set -> [ [X_set], [Y_set] ]\n",
    "        val_set -> [ [X_set], [Y_set] ]\n",
    "    \"\"\"\n",
    "    def __init__(self, epoch: int, batch_size: int, model, optimizer, loss, train_set, val_set = []):\n",
    "        self.epoch = epoch;\n",
    "        self.batch_size = batch_size;\n",
    "        self.optimizer = optimizer;\n",
    "        self.loss = loss;\n",
    "        self.model = model;\n",
    "        \n",
    "        # Dataset\n",
    "        self.train_set = [\n",
    "            [train_set[0]],\n",
    "            [train_set[1]],\n",
    "        ];\n",
    "\n",
    "        # If the validation is not set, then the validation set will be using training set instead\n",
    "        self.val_set = [\n",
    "            [train_set[0]] if (len(val_set) == 0) else [val_set[0]],\n",
    "            [train_set[1]] if (len(val_set) == 0) else [val_set[1]],\n",
    "        ];\n",
    "\n",
    "\n",
    "        self.average_training_loss = 0.0;\n",
    "        self.validation_loss = 0.0;\n",
    "        self.validation_accuracy = 0.0;\n",
    "\n",
    "        # Train One Step\n",
    "    \n",
    "    def fitting(self):\n",
    "        loss = 0.0;\n",
    "\n",
    "        feed_forward = TrainOneStep(self.model, optimizer = self.optimizer, train_weights = True);\n",
    "\n",
    "        # Start of epoch\n",
    "        for epoch in tqdm(range(self.epoch)):\n",
    "            epoch_loss = 0.0;\n",
    "\n",
    "            # Start of batch training\n",
    "            for batch in range(self.batch_size):\n",
    "                start_dataset_index = batch * self.batch_size;\n",
    "                end_dataset_index = (batch + 1) * self.batch_size;\n",
    "\n",
    "                feature_set = convert_to_tensor(self.train_set[0][start_dataset_index:end_dataset_index]);\n",
    "                label_set = convert_to_tensor(self.train_set[1][start_dataset_index:end_dataset_index]);\n",
    "\n",
    "                training_loss_value = feed_forward(feature_set, label_set);\n",
    "\n",
    "                epoch_loss += training_loss_value;\n",
    "            # End of batch training\n",
    "            \n",
    "            self.average_training_loss = epoch_loss / self.batch_size;\n",
    "\n",
    "            # The loss now count with validation set \n",
    "            loss, accuracy = model.evaluate(self.val_set[0], self.val_set[1]);\n",
    "\n",
    "            self.validation_loss = loss;\n",
    "            self.validation_accuracy = accuracy;\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.epoch} | loss: {self.validation_loss} | accuracy: {self.validation_accuracy}\")\n",
    "\n",
    "        # End of epoch\n",
    "        return self.model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] Conv2d conv1: out_channels : 32 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: ReLU\n",
      "[TLX] Conv2d conv2: out_channels : 64 kernel_size: (1, 1) stride: (1, 1) pad: SAME act: ReLU\n",
      "[TLX] Flatten flat:\n",
      "[TLX] Linear  dense1: 64 ReLU\n",
      "[TLX] Linear  output: 10 Softmax\n",
      "[TLX] Input  _inputlayer_17: (8, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "cnn = CNNModel();\n",
    "cnn.init_build(Input(shape = (8, 28, 28, 1)));\n",
    "\n",
    "forward_propagation = ForwardPropagation(neural_network = cnn);\n",
    "\n",
    "trainer = ManualTrainer(\n",
    "    epoch = 5, \n",
    "    batch_size = 64, \n",
    "    optimizer = Adam(lr = 1e-3),\n",
    "    loss = cross_entropy_seq,\n",
    "    model = forward_propagation,\n",
    "    train_set = [train_images, train_labels],\n",
    "    val_set = [val_images, val_labels]\n",
    ");\n",
    "\n",
    "model = trainer.fitting();\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
